{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdEKgRXXMetv"
      },
      "source": [
        "# Next Word Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gHN_Xs00Met2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvGJy9kCMet5"
      },
      "source": [
        "# Now let’s tokenize the text to create a sequence of words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kOmo_pp9Met6"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JvQLcuUMet7"
      },
      "source": [
        "# Now let’s create input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9zE8bTnfMet8"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PeXuKC5lMet8"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Fo2ldFJdMet9"
      },
      "outputs": [],
      "source": [
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nKj1nQYzMet-"
      },
      "outputs": [],
      "source": [
        "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fBHGIrtzMet_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7d8a05-1f06-4918-d397-7c12483daa14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 17, 100)           820000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8200)              1238200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2208800 (8.43 MB)\n",
            "Trainable params: 2208800 (8.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HuTK_iIMet_"
      },
      "source": [
        "The code above defines the model architecture for the next word prediction model. The ‘Sequential’ model is created, which represents a linear stack of layers. The first layer added to the model is the ‘Embedding’ layer, which is responsible for converting the input sequences into dense vectors of fixed size. It takes three arguments:\n",
        "\n",
        "‘total_words’, which represents the total number of distinct words in the vocabulary;\n",
        "‘100’, which denotes the dimensionality of the word embeddings;\n",
        "and ‘input_length’, which specifies the length of the input sequences.\n",
        "The next layer added is the ‘LSTM’ layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data. It has 150 units, which means it will learn 150 internal representations or memory cells.\n",
        "\n",
        "Finally, the ‘Dense’ layer is added, which is a fully connected layer that produces the output predictions. It has ‘total_words’ units and uses the ‘softmax’ activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence.\n",
        "\n",
        "Now let’s compile and train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BmmxxiGaMet_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12da22e0-2b75-4a17-93ff-7eb9ae06937f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3010/3010 [==============================] - 45s 12ms/step - loss: 6.2324 - accuracy: 0.0770\n",
            "Epoch 2/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 5.5011 - accuracy: 0.1235\n",
            "Epoch 3/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 5.1164 - accuracy: 0.1470\n",
            "Epoch 4/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 4.7874 - accuracy: 0.1668\n",
            "Epoch 5/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 4.4854 - accuracy: 0.1821\n",
            "Epoch 6/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 4.1975 - accuracy: 0.2042\n",
            "Epoch 7/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 3.9246 - accuracy: 0.2298\n",
            "Epoch 8/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 3.6628 - accuracy: 0.2603\n",
            "Epoch 9/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 3.4187 - accuracy: 0.2927\n",
            "Epoch 10/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 3.1914 - accuracy: 0.3272\n",
            "Epoch 11/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 2.9798 - accuracy: 0.3642\n",
            "Epoch 12/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 2.7859 - accuracy: 0.3968\n",
            "Epoch 13/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 2.6070 - accuracy: 0.4321\n",
            "Epoch 14/100\n",
            "3010/3010 [==============================] - 25s 8ms/step - loss: 2.4424 - accuracy: 0.4638\n",
            "Epoch 15/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 2.2912 - accuracy: 0.4940\n",
            "Epoch 16/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 2.1529 - accuracy: 0.5215\n",
            "Epoch 17/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 2.0231 - accuracy: 0.5499\n",
            "Epoch 18/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.9081 - accuracy: 0.5728\n",
            "Epoch 19/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 1.8014 - accuracy: 0.5954\n",
            "Epoch 20/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.7027 - accuracy: 0.6164\n",
            "Epoch 21/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.6132 - accuracy: 0.6348\n",
            "Epoch 22/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 1.5312 - accuracy: 0.6539\n",
            "Epoch 23/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.4542 - accuracy: 0.6705\n",
            "Epoch 24/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.3845 - accuracy: 0.6865\n",
            "Epoch 25/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 1.3207 - accuracy: 0.6998\n",
            "Epoch 26/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.2593 - accuracy: 0.7139\n",
            "Epoch 27/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.2092 - accuracy: 0.7235\n",
            "Epoch 28/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.1578 - accuracy: 0.7365\n",
            "Epoch 29/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 1.1113 - accuracy: 0.7465\n",
            "Epoch 30/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.0695 - accuracy: 0.7564\n",
            "Epoch 31/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 1.0270 - accuracy: 0.7653\n",
            "Epoch 32/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.9921 - accuracy: 0.7733\n",
            "Epoch 33/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.9596 - accuracy: 0.7798\n",
            "Epoch 34/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.9269 - accuracy: 0.7871\n",
            "Epoch 35/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.8980 - accuracy: 0.7935\n",
            "Epoch 36/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.8731 - accuracy: 0.7980\n",
            "Epoch 37/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.8478 - accuracy: 0.8040\n",
            "Epoch 38/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.8240 - accuracy: 0.8092\n",
            "Epoch 39/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.8031 - accuracy: 0.8137\n",
            "Epoch 40/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.7826 - accuracy: 0.8184\n",
            "Epoch 41/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.7667 - accuracy: 0.8204\n",
            "Epoch 42/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.7465 - accuracy: 0.8254\n",
            "Epoch 43/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.7305 - accuracy: 0.8292\n",
            "Epoch 44/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.7182 - accuracy: 0.8317\n",
            "Epoch 45/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.7025 - accuracy: 0.8349\n",
            "Epoch 46/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.6915 - accuracy: 0.8364\n",
            "Epoch 47/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.6777 - accuracy: 0.8406\n",
            "Epoch 48/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6652 - accuracy: 0.8426\n",
            "Epoch 49/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.6582 - accuracy: 0.8437\n",
            "Epoch 50/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.6463 - accuracy: 0.8460\n",
            "Epoch 51/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6370 - accuracy: 0.8471\n",
            "Epoch 52/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.6317 - accuracy: 0.8470\n",
            "Epoch 53/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.6194 - accuracy: 0.8514\n",
            "Epoch 54/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.6142 - accuracy: 0.8521\n",
            "Epoch 55/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.6051 - accuracy: 0.8538\n",
            "Epoch 56/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.6015 - accuracy: 0.8547\n",
            "Epoch 57/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5947 - accuracy: 0.8551\n",
            "Epoch 58/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5885 - accuracy: 0.8567\n",
            "Epoch 59/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5860 - accuracy: 0.8561\n",
            "Epoch 60/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5773 - accuracy: 0.8581\n",
            "Epoch 61/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5725 - accuracy: 0.8595\n",
            "Epoch 62/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5697 - accuracy: 0.8599\n",
            "Epoch 63/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5650 - accuracy: 0.8609\n",
            "Epoch 64/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5605 - accuracy: 0.8614\n",
            "Epoch 65/100\n",
            "3010/3010 [==============================] - 23s 7ms/step - loss: 0.5555 - accuracy: 0.8627\n",
            "Epoch 66/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5524 - accuracy: 0.8629\n",
            "Epoch 67/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5555 - accuracy: 0.8605\n",
            "Epoch 68/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5436 - accuracy: 0.8641\n",
            "Epoch 69/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5489 - accuracy: 0.8621\n",
            "Epoch 70/100\n",
            "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5443 - accuracy: 0.8638\n",
            "Epoch 71/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5434 - accuracy: 0.8631\n",
            "Epoch 72/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5337 - accuracy: 0.8669\n",
            "Epoch 73/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5349 - accuracy: 0.8652\n",
            "Epoch 74/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5297 - accuracy: 0.8658\n",
            "Epoch 75/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5329 - accuracy: 0.8658\n",
            "Epoch 76/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5276 - accuracy: 0.8662\n",
            "Epoch 77/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5218 - accuracy: 0.8676\n",
            "Epoch 78/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5284 - accuracy: 0.8659\n",
            "Epoch 79/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5234 - accuracy: 0.8662\n",
            "Epoch 80/100\n",
            "3010/3010 [==============================] - 23s 7ms/step - loss: 0.5222 - accuracy: 0.8671\n",
            "Epoch 81/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5192 - accuracy: 0.8676\n",
            "Epoch 82/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5205 - accuracy: 0.8669\n",
            "Epoch 83/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5142 - accuracy: 0.8681\n",
            "Epoch 84/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5150 - accuracy: 0.8677\n",
            "Epoch 85/100\n",
            "3010/3010 [==============================] - 23s 7ms/step - loss: 0.5125 - accuracy: 0.8684\n",
            "Epoch 86/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5123 - accuracy: 0.8680\n",
            "Epoch 87/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5090 - accuracy: 0.8683\n",
            "Epoch 88/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5072 - accuracy: 0.8691\n",
            "Epoch 89/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5037 - accuracy: 0.8709\n",
            "Epoch 90/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5083 - accuracy: 0.8686\n",
            "Epoch 91/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5036 - accuracy: 0.8704\n",
            "Epoch 92/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5076 - accuracy: 0.8690\n",
            "Epoch 93/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5036 - accuracy: 0.8694\n",
            "Epoch 94/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5045 - accuracy: 0.8691\n",
            "Epoch 95/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5026 - accuracy: 0.8681\n",
            "Epoch 96/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.4995 - accuracy: 0.8696\n",
            "Epoch 97/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5023 - accuracy: 0.8686\n",
            "Epoch 98/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5021 - accuracy: 0.8687\n",
            "Epoch 99/100\n",
            "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5051 - accuracy: 0.8670\n",
            "Epoch 100/100\n",
            "3010/3010 [==============================] - 22s 7ms/step - loss: 0.4982 - accuracy: 0.8695\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7829fc6ed2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "93kHjYNXMeuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31eb43b-8cbe-4cd4-aa2b-1fc1c9b1a875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 364ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "I will leave if they are wrong house\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"I will leave if they\"\n",
        "next_words = 3\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx2pU5ReMeuC"
      },
      "source": [
        "# this is how you can build a Next Word Prediction model using Deep Learning and Python programming language."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}